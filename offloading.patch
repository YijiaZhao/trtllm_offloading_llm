diff --git a/tensorrt_llm/_torch/models/modeling_deepseekv3.py b/tensorrt_llm/_torch/models/modeling_deepseekv3.py
index 687b5ab5a..679b4d707 100644
--- a/tensorrt_llm/_torch/models/modeling_deepseekv3.py
+++ b/tensorrt_llm/_torch/models/modeling_deepseekv3.py
@@ -339,11 +339,13 @@ class Deepseekv3MoE(nn.Module):
                  shared_expert_intermediate_size: int,
                  aux_stream_dict: Dict[AuxStreamType, torch.cuda.Stream],
                  dtype: Optional[torch.dtype] = None,
+                 offloading: bool = False,
                  model_config: ModelConfig = ModelConfig()):
         from ..distributed import AllReduce
 
         super().__init__()
         config = model_config.pretrained_config
+        self.offloading = offloading
         self.top_k = top_k
         self.use_dp = model_config.mapping.enable_attention_dp
         self.enable_alltoall = Deepseekv3MoE.should_enable_alltoall(
@@ -370,6 +372,7 @@ class Deepseekv3MoE(nn.Module):
             reduce_results=
             False,  # In both low‑latency and attention‑DP modes, FusedMoE skips the in‑op all‑reduce.
             model_config=model_config,
+            offloading=offloading,
             aux_stream=aux_stream_dict[AuxStreamType.MoeChunkingOverlap],
             enable_alltoall=self.enable_alltoall)
 
@@ -455,7 +458,13 @@ class Deepseekv3MoE(nn.Module):
         return True
 
     def compute_routed_output(self, hidden_states, hidden_states_fp4,
-                              all_rank_num_tokens, cutlass_min_latency_mode):
+                              all_rank_num_tokens, cutlass_min_latency_mode,
+                              offloading: bool = False,
+                              pingpong: int = None,
+                              w3_w1_weight_pingpong: List=[],
+                              w2_weight_pingpong: List=[],
+                              host_weights: dict={},
+                              offloading_stream: torch.cuda.Stream=None):
         # max-throughput
         use_dp_padding = False
         if self.use_dp and self.mapping.tp_size > 1:
@@ -476,15 +485,32 @@ class Deepseekv3MoE(nn.Module):
                     (0, 0, 0, max_num_token - hidden_states.shape[0]))
 
         router_logits = self.gate(hidden_states)
-
-        routed_output = self.experts(hidden_states_fp4 or hidden_states,
-                                     router_logits,
-                                     cutlass_min_latency_mode,
-                                     output_dtype=hidden_states.dtype,
-                                     all_rank_num_tokens=all_rank_num_tokens,
-                                     use_dp_padding=use_dp_padding)
-
-        return routed_output
+        if offloading:        
+            routed_output = self.experts(hidden_states_fp4 or hidden_states,
+                                         router_logits,
+                                         cutlass_min_latency_mode,
+                                         output_dtype=hidden_states.dtype,
+                                         all_rank_num_tokens=all_rank_num_tokens,
+                                         use_dp_padding=use_dp_padding,
+                                         pingpong = pingpong,
+                                         w3_w1_weight_pingpong = w3_w1_weight_pingpong,
+                                         w2_weight_pingpong = w2_weight_pingpong,
+                                         host_weights = host_weights,
+                                         offloading_stream = offloading_stream                                         
+                                        )
+        else:
+            routed_output = self.experts(hidden_states_fp4 or hidden_states,
+                                         router_logits,
+                                         cutlass_min_latency_mode,
+                                         output_dtype=hidden_states.dtype,
+                                         all_rank_num_tokens=all_rank_num_tokens,
+                                         use_dp_padding=use_dp_padding)
+
+        if offloading:
+            routed_output, pingpong = routed_output
+            return routed_output, pingpong
+        else:
+            return routed_output
 
     def forward(
         self,
@@ -493,6 +519,11 @@ class Deepseekv3MoE(nn.Module):
         all_rank_num_tokens: Optional[list[int]] = None,
         final_all_reduce_params: Optional[AllReduceParams] = None,
         cutlass_min_latency_mode: Optional[bool] = False,
+        pingpong: int = None,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
     ) -> torch.Tensor:
         if cutlass_min_latency_mode:
             assert not self.use_dp
@@ -505,15 +536,28 @@ class Deepseekv3MoE(nn.Module):
             return shared_output
 
         def _compute_routed_output():
-            routed_output = self.compute_routed_output(
-                hidden_states, hidden_states_fp4, all_rank_num_tokens,
-                cutlass_min_latency_mode)
+            if self.offloading:
+                routed_output = self.compute_routed_output(
+                    hidden_states, hidden_states_fp4, all_rank_num_tokens,
+                    cutlass_min_latency_mode,
+                    offloading = self.offloading,
+                    pingpong = pingpong,
+                    w3_w1_weight_pingpong = w3_w1_weight_pingpong,
+                    w2_weight_pingpong = w2_weight_pingpong,
+                    host_weights = host_weights,
+                    offloading_stream = offloading_stream)
+            else:
+                routed_output = self.compute_routed_output(
+                    hidden_states, hidden_states_fp4, all_rank_num_tokens,
+                    cutlass_min_latency_mode)
             return routed_output
 
         shared_output, routed_output = maybe_execute_in_parallel(
             _compute_shared_output, _compute_routed_output,
             self.event_dict[EventType.Main],
             self.event_dict[EventType.MoeShared], self.aux_stream)
+        if self.offloading:
+            routed_output, pingpong = routed_output
 
         if cutlass_min_latency_mode:
             return [shared_output, *routed_output]
@@ -526,15 +570,19 @@ class Deepseekv3MoE(nn.Module):
                     final_hidden_states,
                     all_reduce_params=final_all_reduce_params)
 
-            return final_hidden_states
+            if self.offloading:
+                return final_hidden_states, pingpong
+            else:
+                return final_hidden_states
 
 
 class DeepseekV3DecoderLayer(DecoderLayer):
 
     def __init__(self, model_config: ModelConfig[PretrainedConfig],
                  layer_idx: int, aux_stream_dict: Dict[AuxStreamType,
-                                                       torch.cuda.Stream]):
+                                                       torch.cuda.Stream], offloading: bool=False):
         super().__init__()
+        self.offloading = offloading
         self.model_config = model_config
         config = model_config.pretrained_config
 
@@ -580,6 +628,7 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                 self.num_shared_experts,
                 dtype=config.torch_dtype,
                 model_config=model_config,
+                offloading=offloading,
                 aux_stream_dict=aux_stream_dict)
         else:
             block_size = 1
@@ -659,8 +708,19 @@ class DeepseekV3DecoderLayer(DecoderLayer):
         hidden_states: torch.Tensor,
         attn_metadata: AttentionMetadata,
         residual: torch.Tensor,
+        pingpong: int,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
         **kwargs,
     ) -> torch.Tensor:
+        device = torch.device('cuda')
+        if self.layer_idx == 0:
+            with torch.cuda.stream(offloading_stream):
+                w3_w1_weight_pingpong[pingpong] = host_weights["w3_w1_weight"].to(device, non_blocking=True)
+                w2_weight_pingpong[pingpong] = host_weights["w2_weight"].to(device, non_blocking=True)
+
         if residual is None:
             residual = hidden_states
             hidden_states = self.input_layernorm(hidden_states)
@@ -680,19 +740,30 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                 hidden_states=hidden_states,
                 attn_metadata=attn_metadata,
                 residual=residual,
+                pingpong=pingpong,
+                w3_w1_weight_pingpong=w3_w1_weight_pingpong,
+                w2_weight_pingpong=w2_weight_pingpong,
+                host_weights=host_weights,
+                offloading_stream=offloading_stream,
             )
         else:
             assert isinstance(self.mlp, GatedMLP)
-            return self.forward_mlp(
+            hidden_states, residual = self.forward_mlp(
                 hidden_states=hidden_states,
                 residual=residual,
             )
+            return hidden_states, residual, pingpong
 
     def forward_MoE(
         self,
         hidden_states: torch.Tensor,
         attn_metadata: AttentionMetadata,
         residual: torch.Tensor,
+        pingpong: int,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
     ) -> torch.Tensor:
 
         def _run_MoE(hidden_states, hidden_states_fp4):
@@ -704,6 +775,11 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                     enable_allreduce=not (self.fusion_config.POST_MOE_FUSION
                                           or self.mapping.tp_size == 1)),
                 cutlass_min_latency_mode=cutlass_min_latency_mode,
+                pingpong = pingpong,
+                w3_w1_weight_pingpong = w3_w1_weight_pingpong,
+                w2_weight_pingpong = w2_weight_pingpong,
+                host_weights = host_weights,
+                offloading_stream = offloading_stream,
             )
 
         cutlass_min_latency_mode = self._enable_min_latency_mode(
@@ -762,6 +838,9 @@ class DeepseekV3DecoderLayer(DecoderLayer):
 
             hidden_states = _run_MoE(hidden_states, hidden_states_fp4=None)
 
+            if self.offloading:
+                hidden_states, pingpong = hidden_states
+
             if self.fusion_config.POST_MOE_FUSION:
                 hidden_states, residual = self.allreduce(
                     hidden_states,
@@ -776,7 +855,7 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                     hidden_states, residual = self.next_layer_layernorm(
                         hidden_states, residual)
 
-        return hidden_states, residual
+        return hidden_states, residual, pingpong
 
     def forward_mlp(
         self,
@@ -948,14 +1027,48 @@ class DeepseekV3Model(DecoderModel):
             dtype=config.torch_dtype,
         )
 
+        self.offloading_stride = 1
         self.layers = nn.ModuleList([
             DeepseekV3DecoderLayer(model_config, layer_idx,
-                                   self.aux_stream_dict)
+                                   self.aux_stream_dict,
+                                   offloading=True if (layer_idx-config.first_k_dense_replace) % self.offloading_stride == 0 else False)
             for layer_idx in range(config.num_hidden_layers)
         ])
         self.norm = RMSNorm(hidden_size=config.hidden_size,
                             eps=config.rms_norm_eps,
                             dtype=config.torch_dtype)
+        self.offloading_stream = torch.cuda.Stream()
+        self.offloading_layer_idx = []
+        for i in range(len(self.layers)):
+            if isinstance(self.layers[i].mlp, Deepseekv3MoE) and self.layers[i].offloading:
+                if self.offloading_layer_idx == []:
+                    w3_w1_weight_shape = (self.layers[i].mlp.experts.expert_size_per_partition,
+                                          self.layers[i].mlp.experts.intermediate_size_per_partition * 2,
+                                          self.layers[i].mlp.experts.hidden_size)
+                    w2_weight_shape = (
+                        self.layers[i].mlp.experts.expert_size_per_partition,
+                        self.layers[i].mlp.experts.hidden_size,
+                        self.layers[i].mlp.experts.intermediate_size_per_partition,
+                    )
+                    weight_dtype = self.dtype
+                    device = torch.device('cuda')
+                    self.w3_w1_weight_pingpong = [nn.Parameter(torch.empty(w3_w1_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False),
+                                                  nn.Parameter(torch.empty(w3_w1_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False)]
+                    self.w2_weight_pingpong = [nn.Parameter(torch.empty(w2_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False),
+                                               nn.Parameter(torch.empty(w2_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False)]
+
+                    print(w3_w1_weight_shape, w2_weight_shape)
+                if (i-config.first_k_dense_replace)%self.offloading_stride == 0:
+                    self.offloading_layer_idx.append(i)
+        print("offloading layer: ", self.offloading_layer_idx)
 
     def forward(
         self,
@@ -974,12 +1087,24 @@ class DeepseekV3Model(DecoderModel):
 
         hidden_states = inputs_embeds
         residual = None
+        pingpong_ = 0
+        offloading_layer_idx_ = [tmp for tmp in self.offloading_layer_idx]
+        moe_layer_idx = offloading_layer_idx_[0]        
 
         for decoder_layer in self.layers[:self.num_hidden_layers]:
-            hidden_states, residual = decoder_layer(
+            if len(offloading_layer_idx_) > 1:
+                if decoder_layer.layer_idx == offloading_layer_idx_[0]:
+                    del(offloading_layer_idx_[0])
+                    moe_layer_idx = offloading_layer_idx_[0]
+            hidden_states, residual, pingpong_ = decoder_layer(
                 position_ids=position_ids,
                 hidden_states=hidden_states,
                 attn_metadata=attn_metadata,
+                w3_w1_weight_pingpong=self.w3_w1_weight_pingpong,
+                w2_weight_pingpong=self.w2_weight_pingpong,
+                host_weights=self.layers[moe_layer_idx].mlp.experts.host_weights if offloading_layer_idx_ != [] else None,
+                offloading_stream=self.offloading_stream,
+                pingpong=pingpong_,
                 residual=residual,
             )
 
diff --git a/tensorrt_llm/_torch/modules/fused_moe.py b/tensorrt_llm/_torch/modules/fused_moe.py
index 5b2b89d89..1c5334a9e 100755
--- a/tensorrt_llm/_torch/modules/fused_moe.py
+++ b/tensorrt_llm/_torch/modules/fused_moe.py
@@ -281,6 +281,7 @@ class FusedMoE(nn.Module):
         num_experts: int,
         hidden_size: int,
         intermediate_size: int,
+        offloading: bool,
         dtype: Optional[torch.dtype] = None,
         reduce_results: bool = False,
         model_config: ModelConfig = ModelConfig(),
@@ -293,6 +294,7 @@ class FusedMoE(nn.Module):
         from ..distributed import AllReduce
 
         super().__init__()
+        self.offloading = offloading
         self.routing_method = routing_method
         self.num_experts = num_experts
         self.hidden_size = hidden_size
@@ -372,6 +374,7 @@ class FusedMoE(nn.Module):
             model_config.mapping) if enable_alltoall else None
 
         self._weights_created = False
+        self.host_weights = {}
         if not model_config.skip_create_weights_in_init:
             self.create_weights()
 
@@ -710,17 +713,32 @@ class FusedMoE(nn.Module):
                     f"unsupported quantization mode: {qc.quant_mode}")
             self.setup_quant_scales()
 
-        # Fused gate_up_proj (column parallel)
-        w3_w1_weight = nn.Parameter(torch.empty(w3_w1_weight_shape,
-                                                dtype=weight_dtype),
-                                    requires_grad=False)
-        self.register_parameter("w3_w1_weight", w3_w1_weight)
-
-        # down_proj (row parallel)
-        w2_weight = nn.Parameter(torch.empty(w2_weight_shape,
-                                             dtype=weight_dtype),
-                                 requires_grad=False)
-        self.register_parameter("w2_weight", w2_weight)
+        if self.offloading:
+            self.host_weights["w3_w1_weight"] = nn.Parameter(torch.empty(w3_w1_weight_shape,
+            # w3_w1_weight = nn.Parameter(torch.ones(w3_w1_weight_shape,
+                                                    dtype=weight_dtype,
+                                                    # pin_memory=True,
+                                                    device=torch.device('cpu')),
+                                                    requires_grad=False).pin_memory()
+
+            self.host_weights["w2_weight"] = nn.Parameter(torch.empty(w2_weight_shape,
+            # w2_weight = nn.Parameter(torch.ones(w2_weight_shape,
+                                                dtype=weight_dtype,
+                                                # pin_memory=True,
+                                                device=torch.device('cpu')),
+                                                requires_grad=False).pin_memory()
+        else:
+            # Fused gate_up_proj (column parallel)
+            w3_w1_weight = nn.Parameter(torch.empty(w3_w1_weight_shape,
+                                                    dtype=weight_dtype),
+                                        requires_grad=False)
+            self.register_parameter("w3_w1_weight", w3_w1_weight)
+
+            # down_proj (row parallel)
+            w2_weight = nn.Parameter(torch.empty(w2_weight_shape,
+                                                 dtype=weight_dtype),
+                                     requires_grad=False)
+            self.register_parameter("w2_weight", w2_weight)
         self._weights_created = True
 
     def reducescatter_or_allreduce(
@@ -749,6 +767,11 @@ class FusedMoE(nn.Module):
         output_dtype: Optional[torch.dtype] = None,
         all_rank_num_tokens: Optional[List[int]] = None,
         use_dp_padding: Optional[bool] = None,
+        pingpong: int=None,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None
     ) -> torch.Tensor:
         if isinstance(x, Fp4QuantizedTensor):
             assert output_dtype is not None
@@ -758,7 +781,7 @@ class FusedMoE(nn.Module):
 
         use_fp8_block_scaling = False
         use_w4a8_group_scaling = False
-        weight_dtype = self.w3_w1_weight.dtype
+        weight_dtype = self.host_weights["w3_w1_weight"].dtype if self.offloading else self.w3_w1_weight.dtype
 
         token_selected_experts, token_final_scales = self.routing_method.apply(
             router_logits)
@@ -833,6 +856,19 @@ class FusedMoE(nn.Module):
                 x_sf = reswizzle_sf(x_sf, x_row, x_col,
                                     self.scaling_vector_size)
 
+        if self.offloading:
+            offloading_stream.synchronize()
+            w3_w1_weight_device = w3_w1_weight_pingpong[pingpong]
+            w2_weight_device = w2_weight_pingpong[pingpong]
+    
+            # if offloading_layer_idx != []:
+            if host_weights is not None:
+                pingpong = 0 if pingpong == 1 else 1
+                device = torch.device('cuda')
+                with torch.cuda.stream(offloading_stream):
+                    w3_w1_weight_pingpong[pingpong] = host_weights["w3_w1_weight"].to(device, non_blocking=True)
+                    w2_weight_pingpong[pingpong] = host_weights["w2_weight"].to(device, non_blocking=True)
+
         if self.smart_router and not cutlass_min_latency_mode:
             ep_size = self.cluster_size
             ep_rank = self.cluster_rank
@@ -849,8 +885,8 @@ class FusedMoE(nn.Module):
         else:
             ep_size = self.ep_size
             ep_rank = self.ep_rank
-            w3_w1_weight = self.w3_w1_weight
-            w2_weight = self.w2_weight
+            w3_w1_weight = w3_w1_weight_device if self.offloading else self.w3_w1_weight
+            w2_weight = w2_weight_device if self.offloading else self.w2_weight
             cluster_size = self.cluster_size
             cluster_rank = self.cluster_rank
             quant_scales = self.quant_scales
@@ -889,7 +925,7 @@ class FusedMoE(nn.Module):
             final_hidden_states = final_hidden_states[0]
 
         if not self.enable_alltoall:
-            return final_hidden_states
+            return final_hidden_states, pingpong
         else:
             return self.alltoall_combine(final_hidden_states, alltoall_info,
                                          token_count)
@@ -902,6 +938,11 @@ class FusedMoE(nn.Module):
         output_dtype: Optional[torch.dtype] = None,
         all_rank_num_tokens: Optional[List[int]] = None,
         use_dp_padding: Optional[bool] = None,
+        pingpong: int=None,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None
     ) -> torch.Tensor:
         """
         cutlass_min_latency_mode has no effect when trtllm_gen backend is enabled.
@@ -909,7 +950,12 @@ class FusedMoE(nn.Module):
         if self.is_cutlass():
             return self.forward_cutlass(x, router_logits,
                                         cutlass_min_latency_mode, output_dtype,
-                                        all_rank_num_tokens, use_dp_padding)
+                                        all_rank_num_tokens, use_dp_padding,
+                                        pingpong,
+                                        w3_w1_weight_pingpong,
+                                        w2_weight_pingpong,
+                                        host_weights,
+                                        offloading_stream)
         elif self.is_trtllm():
             return self.forward_trtllmgen(x, router_logits)
         else:
@@ -925,6 +971,11 @@ class FusedMoE(nn.Module):
         output_dtype: Optional[torch.dtype] = None,
         all_rank_num_tokens: Optional[List[int]] = None,
         use_dp_padding: Optional[bool] = None,
+        pingpong: int=None,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
     ) -> torch.Tensor:
         assert self.is_cutlass()
 
@@ -955,7 +1006,14 @@ class FusedMoE(nn.Module):
                 cutlass_min_latency_mode,
                 output_dtype,
                 all_rank_num_tokens=all_rank_num_tokens_padded,
+                pingpong=pingpong,
+                w3_w1_weight_pingpong=w3_w1_weight_pingpong,
+                w2_weight_pingpong=w2_weight_pingpong,
+                host_weights=host_weights,
+                offloading_stream=offloading_stream,
                 use_dp_padding=use_dp_padding)
+            if self.offloading:
+                outputs, pingpong = outputs
             outputs = self.reducescatter_or_allreduce(
                 outputs,
                 all_rank_num_tokens=all_rank_num_tokens_padded,
@@ -1006,7 +1064,14 @@ class FusedMoE(nn.Module):
                                 router_logits,
                                 all_rank_num_tokens=all_rank_num_tokens_list[
                                     idx_chunk] if self.use_dp else None,
+                                pingpong=pingpong,
+                                w3_w1_weight_pingpong=w3_w1_weight_pingpong,
+                                w2_weight_pingpong=w2_weight_pingpong,
+                                host_weights=host_weights,
+                                offloading_stream=offloading_stream,
                                 use_dp_padding=use_dp_padding)
+                            if self.offloading:
+                                outputs, pingpong = outputs
                         if idx_chunk > 0:
                             outputs_list[-1] = self.reducescatter_or_allreduce(
                                 outputs_list[-1],
@@ -1019,7 +1084,14 @@ class FusedMoE(nn.Module):
                             router_logits,
                             all_rank_num_tokens=all_rank_num_tokens_list[
                                 idx_chunk] if self.use_dp else None,
+                            pingpong=pingpong,
+                            w3_w1_weight_pingpong=w3_w1_weight_pingpong,
+                            w2_weight_pingpong=w2_weight_pingpong,
+                            host_weights=host_weights,
+                            offloading_stream=offloading_stream,
                             use_dp_padding=use_dp_padding)
+                        if self.offloading:
+                            outputs, pingpong = outputs
                         with torch.cuda.stream(self.aux_stream):
                             outputs_list[-1] = self.reducescatter_or_allreduce(
                                 outputs_list[-1],
@@ -1031,8 +1103,14 @@ class FusedMoE(nn.Module):
                         x,
                         router_logits,
                         all_rank_num_tokens=all_rank_num_tokens_list[idx_chunk]
-                        if self.use_dp else None)
-
+                        if self.use_dp else None,
+                        pingpong=pingpong,
+                        w3_w1_weight_pingpong=w3_w1_weight_pingpong,
+                        w2_weight_pingpong=w2_weight_pingpong,
+                        host_weights=host_weights,
+                        offloading_stream=offloading_stream)
+                    if self.offloading:
+                        outputs, pingpong = outputs
                 outputs_list.append(outputs)
             if not self.enable_alltoall:
                 if num_chunks % 2 == 0:
@@ -1053,7 +1131,10 @@ class FusedMoE(nn.Module):
         if self.use_dp:
             rank = self.mapping.tp_rank
             outputs = outputs[:all_rank_num_tokens[rank]]
-        return outputs
+        if self.offloading:
+            return outputs, pingpong
+        else:
+            return outputs
 
     def forward_trtllmgen(self, x: torch.Tensor,
                           router_logits: torch.Tensor) -> torch.Tensor:
@@ -1217,6 +1298,7 @@ class FusedMoE(nn.Module):
                                      w3_weight,
                                      dst_w3_w1_weight: torch.Tensor,
                                      is_trtllm: bool = False):
+            # print("000000000", dst_w3_w1_weight)
             w1_weight_shard = load_weight_shard(w1_weight, self.tp_size,
                                                 self.tp_rank,
                                                 TensorParallelMode.COLUMN)
@@ -1305,6 +1387,7 @@ class FusedMoE(nn.Module):
 
             thread = threading.Thread(target=load_expert_w3_w1_weight,
                                       args=(w1_weight, w3_weight,
+                                            self.host_weights["w3_w1_weight"].data[expert_idx] if self.offloading else
                                             self.w3_w1_weight.data[expert_idx],
                                             is_trtllm_nvfp4))
             thread.start()
@@ -1312,6 +1395,7 @@ class FusedMoE(nn.Module):
 
             thread = threading.Thread(target=load_expert_w2_weight,
                                       args=(w2_weight,
+                                            self.host_weights["w2_weight"].data[expert_idx] if self.offloading else
                                             self.w2_weight.data[expert_idx],
                                             is_trtllm_nvfp4))
             thread.start()
