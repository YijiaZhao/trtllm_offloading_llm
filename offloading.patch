diff --git a/tensorrt_llm/_torch/models/modeling_deepseekv3.py b/tensorrt_llm/_torch/models/modeling_deepseekv3.py
index 96a76deb5..aa00dd3aa 100644
--- a/tensorrt_llm/_torch/models/modeling_deepseekv3.py
+++ b/tensorrt_llm/_torch/models/modeling_deepseekv3.py
@@ -61,7 +61,8 @@ from ..utils import (AuxStreamType, EventType, Fp4QuantizedTensor,
                      disable_fp4_allgather)
 from .modeling_utils import (DecoderModel, DecoderModelForCausalLM,
                              EagerFusionConfig, register_auto_model)
-
+from ...quantization.utils.fp4_utils import float4_sf_dtype
+FUSED_MOE_NVFP4_WEIGHT_DTYPE = torch.int64
 
 @triton.jit
 def weight_dequant_kernel(x_ptr, s_ptr, y_ptr, M, N, BLOCK_SIZE: tl.constexpr):
@@ -339,11 +340,13 @@ class Deepseekv3MoE(nn.Module):
                  shared_expert_intermediate_size: int,
                  aux_stream_dict: Dict[AuxStreamType, torch.cuda.Stream],
                  dtype: Optional[torch.dtype] = None,
+                 offloading: bool = False,
                  model_config: ModelConfig = ModelConfig()):
         from ..distributed import AllReduce
 
         super().__init__()
         config = model_config.pretrained_config
+        self.offloading = offloading
         self.top_k = top_k
         self.use_dp = model_config.mapping.enable_attention_dp
         self.enable_alltoall = Deepseekv3MoE.should_enable_alltoall(
@@ -370,6 +373,7 @@ class Deepseekv3MoE(nn.Module):
             reduce_results=
             False,  # In both low‑latency and attention‑DP modes, FusedMoE skips the in‑op all‑reduce.
             model_config=model_config,
+            offloading=offloading,
             aux_stream=aux_stream_dict[AuxStreamType.MoeChunkingOverlap],
             enable_alltoall=self.enable_alltoall)
 
@@ -455,7 +459,14 @@ class Deepseekv3MoE(nn.Module):
         return True
 
     def compute_routed_output(self, hidden_states, hidden_states_fp4,
-                              all_rank_num_tokens, cutlass_min_latency_mode):
+                              all_rank_num_tokens, cutlass_min_latency_mode,
+                              offloading: bool = False,
+                              pingpong: int = None,
+                              w3_w1_weight_pingpong: List=[],
+                              w2_weight_pingpong: List=[],
+                              host_weights: dict={},
+                              offloading_stream: torch.cuda.Stream=None,
+                              event: torch.cuda.Event=None):
         # max-throughput
         use_dp_padding = False
         if self.use_dp and self.mapping.tp_size > 1:
@@ -476,15 +487,32 @@ class Deepseekv3MoE(nn.Module):
                     (0, 0, 0, max_num_token - hidden_states.shape[0]))
 
         router_logits = self.gate(hidden_states)
-
-        routed_output = self.experts(hidden_states_fp4 or hidden_states,
-                                     router_logits,
-                                     cutlass_min_latency_mode,
-                                     output_dtype=hidden_states.dtype,
-                                     all_rank_num_tokens=all_rank_num_tokens,
-                                     use_dp_padding=use_dp_padding)
-
-        return routed_output
+        if offloading:        
+            routed_output = self.experts(hidden_states_fp4 or hidden_states,
+                                         router_logits,
+                                         cutlass_min_latency_mode,
+                                         output_dtype=hidden_states.dtype,
+                                         all_rank_num_tokens=all_rank_num_tokens,
+                                         use_dp_padding=use_dp_padding,
+                                         pingpong = pingpong,
+                                         w3_w1_weight_pingpong = w3_w1_weight_pingpong,
+                                         w2_weight_pingpong = w2_weight_pingpong,
+                                         host_weights = host_weights,
+                                         offloading_stream = offloading_stream,
+                                         event = event)
+        else:
+            routed_output = self.experts(hidden_states_fp4 or hidden_states,
+                                         router_logits,
+                                         cutlass_min_latency_mode,
+                                         output_dtype=hidden_states.dtype,
+                                         all_rank_num_tokens=all_rank_num_tokens,
+                                         use_dp_padding=use_dp_padding)
+
+        if offloading:
+            routed_output, pingpong, event = routed_output
+            return routed_output, pingpong, event
+        else:
+            return routed_output
 
     def forward(
         self,
@@ -493,6 +521,12 @@ class Deepseekv3MoE(nn.Module):
         all_rank_num_tokens: Optional[list[int]] = None,
         final_all_reduce_params: Optional[AllReduceParams] = None,
         cutlass_min_latency_mode: Optional[bool] = False,
+        pingpong: int = None,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
+        event: torch.cuda.Event=None,
     ) -> torch.Tensor:
         if cutlass_min_latency_mode:
             assert not self.use_dp
@@ -505,15 +539,29 @@ class Deepseekv3MoE(nn.Module):
             return shared_output
 
         def _compute_routed_output():
-            routed_output = self.compute_routed_output(
-                hidden_states, hidden_states_fp4, all_rank_num_tokens,
-                cutlass_min_latency_mode)
+            if self.offloading:
+                routed_output = self.compute_routed_output(
+                    hidden_states, hidden_states_fp4, all_rank_num_tokens,
+                    cutlass_min_latency_mode,
+                    offloading = self.offloading,
+                    pingpong = pingpong,
+                    w3_w1_weight_pingpong = w3_w1_weight_pingpong,
+                    w2_weight_pingpong = w2_weight_pingpong,
+                    host_weights = host_weights,
+                    offloading_stream = offloading_stream,
+                    event = event)
+            else:
+                routed_output = self.compute_routed_output(
+                    hidden_states, hidden_states_fp4, all_rank_num_tokens,
+                    cutlass_min_latency_mode)
             return routed_output
 
         shared_output, routed_output = maybe_execute_in_parallel(
             _compute_shared_output, _compute_routed_output,
             self.event_dict[EventType.Main],
             self.event_dict[EventType.MoeShared], self.aux_stream)
+        if self.offloading:
+            routed_output, pingpong, event = routed_output
 
         if cutlass_min_latency_mode:
             return [shared_output, *routed_output]
@@ -526,15 +574,19 @@ class Deepseekv3MoE(nn.Module):
                     final_hidden_states,
                     all_reduce_params=final_all_reduce_params)
 
-            return final_hidden_states
+            if self.offloading:
+                return final_hidden_states, pingpong, event
+            else:
+                return final_hidden_states
 
 
 class DeepseekV3DecoderLayer(DecoderLayer):
 
     def __init__(self, model_config: ModelConfig[PretrainedConfig],
                  layer_idx: int, aux_stream_dict: Dict[AuxStreamType,
-                                                       torch.cuda.Stream]):
+                                                       torch.cuda.Stream], offloading: bool=False):
         super().__init__()
+        self.offloading = offloading
         self.model_config = model_config
         config = model_config.pretrained_config
 
@@ -580,6 +632,7 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                 self.num_shared_experts,
                 dtype=config.torch_dtype,
                 model_config=model_config,
+                offloading=offloading,
                 aux_stream_dict=aux_stream_dict)
         else:
             block_size = 1
@@ -660,8 +713,21 @@ class DeepseekV3DecoderLayer(DecoderLayer):
         hidden_states: torch.Tensor,
         attn_metadata: AttentionMetadata,
         residual: torch.Tensor,
+        pingpong: int,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
+        event: torch.cuda.Event=None,
         **kwargs,
     ) -> torch.Tensor:
+        if self.layer_idx == 0 and self.offloading:
+            assert event is not None
+            with torch.cuda.stream(offloading_stream):
+                w3_w1_weight_pingpong[pingpong].copy_(host_weights["w3_w1_weight"], non_blocking=True)
+                w2_weight_pingpong[pingpong].copy_(host_weights["w2_weight"], non_blocking=True)
+                event.record(offloading_stream)
+
         if residual is None:
             residual = hidden_states
             hidden_states = self.input_layernorm(hidden_states)
@@ -680,19 +746,32 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                 hidden_states=hidden_states,
                 attn_metadata=attn_metadata,
                 residual=residual,
+                pingpong=pingpong,
+                w3_w1_weight_pingpong=w3_w1_weight_pingpong,
+                w2_weight_pingpong=w2_weight_pingpong,
+                host_weights=host_weights,
+                offloading_stream=offloading_stream,
+                event=event,
             )
         else:
             assert isinstance(self.mlp, GatedMLP)
-            return self.forward_mlp(
+            hidden_states, residual = self.forward_mlp(
                 hidden_states=hidden_states,
                 residual=residual,
             )
+            return hidden_states, residual, pingpong, event
 
     def forward_MoE(
         self,
         hidden_states: torch.Tensor,
         attn_metadata: AttentionMetadata,
         residual: torch.Tensor,
+        pingpong: int,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
+        event: torch.cuda.Event=None,
     ) -> torch.Tensor:
 
         def _run_MoE(hidden_states, hidden_states_fp4):
@@ -704,6 +783,12 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                     enable_allreduce=not (self.fusion_config.POST_MOE_FUSION
                                           or self.mapping.tp_size == 1)),
                 cutlass_min_latency_mode=cutlass_min_latency_mode,
+                pingpong = pingpong,
+                w3_w1_weight_pingpong = w3_w1_weight_pingpong,
+                w2_weight_pingpong = w2_weight_pingpong,
+                host_weights = host_weights,
+                offloading_stream = offloading_stream,
+                event = event,
             )
 
         cutlass_min_latency_mode = self._enable_min_latency_mode(
@@ -762,6 +847,8 @@ class DeepseekV3DecoderLayer(DecoderLayer):
 
             hidden_states = _run_MoE(hidden_states, hidden_states_fp4=None)
 
+            if self.offloading:
+                hidden_states, pingpong, event = hidden_states
             if self.fusion_config.POST_MOE_FUSION:
                 hidden_states, residual = self.allreduce(
                     hidden_states,
@@ -776,7 +863,7 @@ class DeepseekV3DecoderLayer(DecoderLayer):
                     hidden_states, residual = self.next_layer_layernorm(
                         hidden_states, residual)
 
-        return hidden_states, residual
+        return hidden_states, residual, pingpong, event
 
     def forward_mlp(
         self,
@@ -952,15 +1039,94 @@ class DeepseekV3Model(DecoderModel):
             dtype=config.torch_dtype,
         )
 
-        self.layers = nn.ModuleList([
-            DeepseekV3DecoderLayer(model_config, layer_idx,
-                                   self.aux_stream_dict)
-            for layer_idx in range(config.num_hidden_layers)
-        ])
+        self.offloading_stride = 2
+        self.loading_stride = 0
+        assert (self.offloading_stride != 0 or self.loading_stride != 0)
+        if self.offloading_stride > 0:
+            self.layers = nn.ModuleList([
+                DeepseekV3DecoderLayer(model_config, layer_idx,
+                                       self.aux_stream_dict,
+                                       offloading=True if (layer_idx-config.first_k_dense_replace) % self.offloading_stride == 0 else False if layer_idx>=config.first_k_dense_replace else False)
+                for layer_idx in range(config.num_hidden_layers)
+            ])
+        else:
+            self.layers = nn.ModuleList([
+                DeepseekV3DecoderLayer(model_config, layer_idx,
+                                       self.aux_stream_dict,
+                                       offloading=False if (layer_idx-config.first_k_dense_replace + 1) % self.loading_stride == 0 else True if layer_idx>=config.first_k_dense_replace else False)
+                for layer_idx in range(config.num_hidden_layers)
+            ])
         self.norm = RMSNorm(hidden_size=config.hidden_size,
                             eps=config.rms_norm_eps,
                             dtype=config.torch_dtype)
 
+        device = torch.cuda.current_device()
+        self.offloading_stream = torch.cuda.Stream(device=device)
+
+        self.offloading_layer_idx = []
+        for i in range(len(self.layers)):
+            if isinstance(self.layers[i].mlp, Deepseekv3MoE) and self.layers[i].offloading:
+                fused_moe = self.layers[i].mlp.experts
+                qm = fused_moe.quant_config.quant_mode
+
+                if self.offloading_layer_idx == []:
+                    w3_w1_weight_shape = (fused_moe.expert_size_per_partition,
+                                          fused_moe.intermediate_size_per_partition * 2,
+                                          fused_moe.hidden_size)
+                    w2_weight_shape = (
+                        fused_moe.expert_size_per_partition,
+                        fused_moe.hidden_size,
+                        fused_moe.intermediate_size_per_partition,
+                    )
+                    weight_dtype = fused_moe.dtype
+
+                    if (qm.has_fp8_qdq() or qm.has_fp8_block_scales()):
+                        weight_dtype = torch.float8_e4m3fn
+                    elif qm.is_int4_weight_only_per_group:
+                        weight_dtype = torch.int8
+                        w3_w1_weight_shape = (
+                            fused_moe.expert_size_per_partition,
+                            fused_moe.intermediate_size_per_partition * 2,
+                            fused_moe.hidden_size // 2
+                        )
+                        w2_weight_shape = (
+                            fused_moe.expert_size_per_partition,
+                            fused_moe.hidden_size,
+                            fused_moe.intermediate_size_per_partition // 2
+                        )
+                    elif qm.has_nvfp4():
+                        if self.layers[i].mlp.experts.is_trtllm():
+                            weight_dtype = float4_sf_dtype
+                            weight_vec_size = torch.iinfo(weight_dtype).bits // 4
+                        else:
+                            weight_dtype = FUSED_MOE_NVFP4_WEIGHT_DTYPE
+                            weight_vec_size = torch.iinfo(weight_dtype).bits // 4
+
+                        w3_w1_weight_shape = (self.expert_size_per_partition,
+                                              self.intermediate_size_per_partition * 2,
+                                              self.hidden_size // weight_vec_size)
+                        w2_weight_shape = (self.expert_size_per_partition,
+                                           self.hidden_size,
+                                           self.intermediate_size_per_partition //
+                                           weight_vec_size)
+
+                    # device = torch.device('cuda')
+                    self.w3_w1_weight_pingpong = [nn.Parameter(torch.empty(w3_w1_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False),
+                                                  nn.Parameter(torch.empty(w3_w1_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False)]
+                    self.w2_weight_pingpong = [nn.Parameter(torch.empty(w2_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False),
+                                               nn.Parameter(torch.empty(w2_weight_shape,
+                                                               dtype=weight_dtype,
+                                                               device=device), requires_grad=False)]
+                self.offloading_layer_idx.append(i)
+                    
+        print("offloading layer: ", self.offloading_layer_idx)
+
     def forward(
         self,
         attn_metadata: AttentionMetadata,
@@ -978,13 +1144,27 @@ class DeepseekV3Model(DecoderModel):
 
         hidden_states = inputs_embeds
         residual = None
+        pingpong_ = 0
+        offloading_layer_idx_ = [tmp for tmp in self.offloading_layer_idx]
+        moe_layer_idx = offloading_layer_idx_[0]
+        event = torch.cuda.Event()
 
         for decoder_layer in self.layers[:self.num_hidden_layers]:
-            hidden_states, residual = decoder_layer(
+            if len(offloading_layer_idx_) > 1:
+                if decoder_layer.layer_idx == offloading_layer_idx_[0]:
+                    del(offloading_layer_idx_[0])
+                    moe_layer_idx = offloading_layer_idx_[0]
+            hidden_states, residual, pingpong_, event = decoder_layer(
                 position_ids=position_ids,
                 hidden_states=hidden_states,
                 attn_metadata=attn_metadata,
+                w3_w1_weight_pingpong=self.w3_w1_weight_pingpong,
+                w2_weight_pingpong=self.w2_weight_pingpong,
+                host_weights=self.layers[moe_layer_idx].mlp.experts.host_weights if offloading_layer_idx_ != [] else None,
+                offloading_stream=self.offloading_stream,
+                pingpong=pingpong_,
                 residual=residual,
+                event=event,
             )
 
         return hidden_states
diff --git a/tensorrt_llm/_torch/modules/fused_moe.py b/tensorrt_llm/_torch/modules/fused_moe.py
index 2fbd2d28a..247988ec3 100755
--- a/tensorrt_llm/_torch/modules/fused_moe.py
+++ b/tensorrt_llm/_torch/modules/fused_moe.py
@@ -359,6 +359,7 @@ class FusedMoE(nn.Module):
         num_experts: int,
         hidden_size: int,
         intermediate_size: int,
+        offloading: bool,
         dtype: Optional[torch.dtype] = None,
         reduce_results: bool = False,
         model_config: ModelConfig = ModelConfig(),
@@ -371,6 +372,7 @@ class FusedMoE(nn.Module):
         from ..distributed import AllReduce
 
         super().__init__()
+        self.offloading = offloading
         self.routing_method = routing_method
         self.num_experts = num_experts
         self.hidden_size = hidden_size
@@ -464,6 +466,7 @@ class FusedMoE(nn.Module):
             model_config.mapping) if enable_alltoall else None
 
         self._weights_created = False
+        self.host_weights = {}
         if not model_config.skip_create_weights_in_init:
             self.create_weights()
 
@@ -801,18 +804,32 @@ class FusedMoE(nn.Module):
                 raise ValueError(
                     f"unsupported quantization mode: {qc.quant_mode}")
             self.setup_quant_scales()
-
-        # Fused gate_up_proj (column parallel)
-        w3_w1_weight = nn.Parameter(torch.empty(w3_w1_weight_shape,
-                                                dtype=weight_dtype),
-                                    requires_grad=False)
-        self.register_parameter("w3_w1_weight", w3_w1_weight)
-
-        # down_proj (row parallel)
-        w2_weight = nn.Parameter(torch.empty(w2_weight_shape,
-                                             dtype=weight_dtype),
-                                 requires_grad=False)
-        self.register_parameter("w2_weight", w2_weight)
+        if self.offloading:
+            self.host_weights["w3_w1_weight"] = nn.Parameter(torch.empty(w3_w1_weight_shape,
+            # w3_w1_weight = nn.Parameter(torch.ones(w3_w1_weight_shape,
+                                                    dtype=weight_dtype,
+                                                    # pin_memory=True,
+                                                    device=torch.device('cpu')),
+                                                    requires_grad=False).pin_memory()
+
+            self.host_weights["w2_weight"] = nn.Parameter(torch.empty(w2_weight_shape,
+            # w2_weight = nn.Parameter(torch.ones(w2_weight_shape,
+                                                dtype=weight_dtype,
+                                                # pin_memory=True,
+                                                device=torch.device('cpu')),
+                                                requires_grad=False).pin_memory()
+        else:
+            # Fused gate_up_proj (column parallel)
+            w3_w1_weight = nn.Parameter(torch.empty(w3_w1_weight_shape,
+                                                    dtype=weight_dtype),
+                                        requires_grad=False)
+            self.register_parameter("w3_w1_weight", w3_w1_weight)
+
+            # down_proj (row parallel)
+            w2_weight = nn.Parameter(torch.empty(w2_weight_shape,
+                                                 dtype=weight_dtype),
+                                     requires_grad=False)
+            self.register_parameter("w2_weight", w2_weight)
         self._weights_created = True
 
     def reducescatter_or_allreduce(
@@ -841,6 +858,8 @@ class FusedMoE(nn.Module):
         output_dtype: Optional[torch.dtype] = None,
         all_rank_num_tokens: Optional[List[int]] = None,
         use_dp_padding: Optional[bool] = None,
+        w3_w1_weight_device: Optional[torch.Tensor] = None,
+        w2_weight_device: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
         if isinstance(x, Fp4QuantizedTensor):
             assert output_dtype is not None
@@ -850,7 +869,7 @@ class FusedMoE(nn.Module):
 
         use_fp8_block_scaling = False
         use_w4a8_group_scaling = False
-        weight_dtype = self.w3_w1_weight.dtype
+        weight_dtype = self.host_weights["w3_w1_weight"].dtype if self.offloading else self.w3_w1_weight.dtype
 
         token_selected_experts, token_final_scales = self.routing_method.apply(
             router_logits)
@@ -931,18 +950,24 @@ class FusedMoE(nn.Module):
             expert_start = ep_rank * self.num_experts // ep_size
             expert_end = min(self.num_experts,
                              (ep_rank + 1) * self.num_experts // ep_size)
-            w3_w1_weight = self.w3_w1_weight.narrow(0, expert_start,
-                                                    expert_end - expert_start)
-            w2_weight = self.w2_weight.narrow(0, expert_start,
-                                              expert_end - expert_start)
+            if self.offloading:
+                w3_w1_weight = w3_w1_weight_device.narrow(0, expert_start,
+                                                        expert_end - expert_start)
+                w2_weight = w2_weight_device.narrow(0, expert_start,
+                                                  expert_end - expert_start)                
+            else:
+                w3_w1_weight = self.w3_w1_weight.narrow(0, expert_start,
+                                                        expert_end - expert_start)
+                w2_weight = self.w2_weight.narrow(0, expert_start,
+                                                  expert_end - expert_start)
             cluster_size = self.ep_size
             cluster_rank = self.ep_rank
             quant_scales = self.get_quant_scales(expert_start, expert_end)
         else:
             ep_size = self.ep_size
             ep_rank = self.ep_rank
-            w3_w1_weight = self.w3_w1_weight
-            w2_weight = self.w2_weight
+            w3_w1_weight = w3_w1_weight_device if self.offloading else self.w3_w1_weight
+            w2_weight = w2_weight_device if self.offloading else self.w2_weight
             cluster_size = self.cluster_size
             cluster_rank = self.cluster_rank
             quant_scales = self.quant_scales
@@ -994,6 +1019,12 @@ class FusedMoE(nn.Module):
         output_dtype: Optional[torch.dtype] = None,
         all_rank_num_tokens: Optional[List[int]] = None,
         use_dp_padding: Optional[bool] = None,
+        pingpong: int=None,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
+        event: torch.cuda.Event=None,
     ) -> torch.Tensor:
         """
         cutlass_min_latency_mode has no effect when trtllm_gen backend is enabled.
@@ -1001,7 +1032,13 @@ class FusedMoE(nn.Module):
         if self.is_cutlass():
             return self.forward_cutlass(x, router_logits,
                                         cutlass_min_latency_mode, output_dtype,
-                                        all_rank_num_tokens, use_dp_padding)
+                                        all_rank_num_tokens, use_dp_padding,
+                                        pingpong,
+                                        w3_w1_weight_pingpong,
+                                        w2_weight_pingpong,
+                                        host_weights,
+                                        offloading_stream,
+                                        event=event)
         elif self.is_trtllm():
             return self.forward_trtllmgen(x, router_logits)
         else:
@@ -1017,9 +1054,16 @@ class FusedMoE(nn.Module):
         output_dtype: Optional[torch.dtype] = None,
         all_rank_num_tokens: Optional[List[int]] = None,
         use_dp_padding: Optional[bool] = None,
+        pingpong: int=None,
+        w3_w1_weight_pingpong: List=[],
+        w2_weight_pingpong: List=[],
+        host_weights: dict={},
+        offloading_stream: torch.cuda.Stream=None,
+        event: torch.cuda.Event=None,
     ) -> torch.Tensor:
         assert self.is_cutlass()
-
+        w3_w1_weight_device = None
+        w2_weight_device = None
         if self.use_dp:
             assert all_rank_num_tokens is not None
             assert use_dp_padding is not None
@@ -1040,6 +1084,21 @@ class FusedMoE(nn.Module):
                                           ] * len(all_rank_num_tokens)
         else:
             all_rank_num_tokens_padded = all_rank_num_tokens
+
+        if self.offloading:
+            with torch.cuda.stream(stream=offloading_stream):
+                event.wait(torch.cuda.current_stream())
+                w3_w1_weight_device = w3_w1_weight_pingpong[pingpong]
+                w2_weight_device = w2_weight_pingpong[pingpong]
+                event_main_stream = torch.cuda.Event()
+                event_main_stream.record(torch.cuda.current_stream())
+                if host_weights is not {}:
+                    pingpong = 0 if pingpong == 1 else 1
+                    event_main_stream.wait(offloading_stream)
+                    w3_w1_weight_pingpong[pingpong].copy_(host_weights["w3_w1_weight"], non_blocking=True)
+                    w2_weight_pingpong[pingpong].copy_(host_weights["w2_weight"], non_blocking=True)
+                    event.record(offloading_stream)
+
         if num_chunks == 1:
             outputs = self.forward_chunk(
                 x,
@@ -1047,6 +1106,8 @@ class FusedMoE(nn.Module):
                 cutlass_min_latency_mode,
                 output_dtype,
                 all_rank_num_tokens=all_rank_num_tokens_padded,
+                w3_w1_weight_device=w3_w1_weight_device,
+                w2_weight_device=w2_weight_device,
                 use_dp_padding=use_dp_padding)
             outputs = self.reducescatter_or_allreduce(
                 outputs,
@@ -1098,6 +1159,8 @@ class FusedMoE(nn.Module):
                                 router_logits,
                                 all_rank_num_tokens=all_rank_num_tokens_list[
                                     idx_chunk] if self.use_dp else None,
+                                w3_w1_weight_device=w3_w1_weight_device,
+                                w2_weight_device=w2_weight_device,
                                 use_dp_padding=use_dp_padding)
                         if idx_chunk > 0:
                             outputs_list[-1] = self.reducescatter_or_allreduce(
@@ -1111,6 +1174,8 @@ class FusedMoE(nn.Module):
                             router_logits,
                             all_rank_num_tokens=all_rank_num_tokens_list[
                                 idx_chunk] if self.use_dp else None,
+                            w3_w1_weight_device=w3_w1_weight_device,
+                            w2_weight_device=w2_weight_device,
                             use_dp_padding=use_dp_padding)
                         with torch.cuda.stream(self.aux_stream):
                             outputs_list[-1] = self.reducescatter_or_allreduce(
@@ -1123,8 +1188,9 @@ class FusedMoE(nn.Module):
                         x,
                         router_logits,
                         all_rank_num_tokens=all_rank_num_tokens_list[idx_chunk]
-                        if self.use_dp else None)
-
+                        if self.use_dp else None,
+                        w3_w1_weight_device=w3_w1_weight_device,
+                        w2_weight_device=w2_weight_device)
                 outputs_list.append(outputs)
             if not self.enable_alltoall:
                 if num_chunks % 2 == 0:
@@ -1145,7 +1211,10 @@ class FusedMoE(nn.Module):
         if self.use_dp:
             rank = self.mapping.tp_rank
             outputs = outputs[:all_rank_num_tokens[rank]]
-        return outputs
+        if self.offloading:
+            return outputs, pingpong, event
+        else:
+            return outputs
 
     def forward_trtllmgen(self, x: torch.Tensor,
                           router_logits: torch.Tensor) -> torch.Tensor:
@@ -1445,6 +1514,7 @@ class FusedMoE(nn.Module):
 
             thread = threading.Thread(target=load_expert_w3_w1_weight,
                                       args=(w1_weight, w3_weight,
+                                            self.host_weights["w3_w1_weight"].data[expert_idx] if self.offloading else
                                             self.w3_w1_weight.data[expert_idx],
                                             is_trtllm_nvfp4))
             thread.start()
@@ -1452,6 +1522,7 @@ class FusedMoE(nn.Module):
 
             thread = threading.Thread(target=load_expert_w2_weight,
                                       args=(w2_weight,
+                                            self.host_weights["w2_weight"].data[expert_idx] if self.offloading else
                                             self.w2_weight.data[expert_idx],
                                             is_trtllm_nvfp4))
             thread.start()
